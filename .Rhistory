setwd("D:/Statistics/Resampling Methods")
alpha.fn <- fuction(data, index){}
alpha.fn <- fuction(data, index){
alpha.fn <- function(data, index){
X = data$X[index]
Y = data$Y[index]
return( (var(Y)-cov(X, Y))/( var(X)+var(Y)-2*cov(X, Y)) )
}
boot(Portfolio, alpha.fn, R = 1000)
library(boot)
boot(Portfolio, alpha.fn, R = 1000)
library(ISLR)
boot(Portfolio, alpha.fn, R = 1000)
alpha.fn(Portfolio, 1:100)
X = Portfolio$X[1:100]
coef(lm(mpg~horsepower, data = Auto))
boot.fn <- function(data, index)
return(coef(lm(mpg~horsepower, data = data[index,])))
boot.fn(Auto, 1:392)
boot(Auto, boot.fn, 1000)
boot.fn <- function(data, index)
return(coef(lm(mpg~horsepower, data = data[index,])))
boot.fn <- function(data, index)
return(coef(lm(mpg~poly(horsepower, 2), data = data[index,])))
boot.fn(Auto, 1:392)
boot(Auto, boot.fn, R = 1000)
summary(lm(mpg~poly(horsepower, 2), data = Auto))$coef
boot.fn <- function(data, index)
return(coef(lm(mpg~poly(horsepower^2), data = data[index,])))
boot.fn(Auto, 1:392)
boot(Auto, boot.fn, R = 1000)
summary(lm(mpg~poly(horsepower^2), data = Auto))$coef
# Cross Validation
# Import libraries:
library(ISLR)
library(tidyverse)
library(caret)
# Data: The Auto data in the ISLR package
data <- Auto
# Method 1: Validation Set Approach
# _________________________________
# Split data into train data and validation data sets, training set and validation set:
set.seed(1)
train <- data$mpg %>%
createDataPartition(p = 0.65, list = FALSE)  # method 1
# train = sample(1:nrow(data), size = nrow(data)*0.65, replace = FALSE ) # method 2
# Note samples will be different in individual observations selected.
train_set <- Auto[train, ] # The training set
validation_set <- Auto[-train, ] # The validation set
# Fit a linear regression model on the training set:
lm.fit <- lm(mpg~horsepower, data = train_set)
# Use the model to predict values using the fitted model:
predict <- lm.fit %>% predict(validation_set)
# predict <- predict(lm.fit, validation_set) In case you used method 2
# Model Performance Metrics:
data.frame(R2 = R2(predict, validation_set$mpg),
RMSE = RMSE(predict, validation_set$mpg),
MAE = MAE(predict, validation_set$mpg),
PER = RMSE(predict, validation_set$mpg)/mean(validation_set$mpg))
# Note: The higher the R2, the lower the RMSE and MAE the better the model.
# Fitting a polynomial to compare against the linear model:
lm.fit1 <- lm(mpg~poly(horsepower, 2), data = train_set )
predict1 <- lm.fit1 %>% predict(validation_set)
# Model metrics
data.frame(R2 = R2(predict1, validation_set$mpg),
RMSE = RMSE(predict1, validation_set$mpg),
MAE = MAE(predict1, validation_set$mpg),
PER = RMSE(predict1, validation_set$mpg)/mean(validation_set$mpg))
# Comparing, the quadratic fit(lm.fit1) is a better model than the linear fit(lm.fit)
# A disadvantage the data split is that we build a model on a fraction of the data set only, possibly leaving out some interesting information about data.
# This leads to higher bias.
# Method 2:Leave-One-Out Cross-Validation (LOOCV)
# _______________________________________________
# LOOCV addresses the Validation set approach drawback.
# The LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions
library(boot)
glm.fit <- glm(mpg~horsepower, data = data)
cv.err <- cv.glm(data, glm.fit)
cv.err$delta # Cross-validation results.
# Fit increasing complex polynomial fits by the for loop
cv.err = rep(0, 5)
for(i in 1:5){
glm.fit <- glm(mpg~poly(horsepower, i), data = data)
cv.err[i] <- cv.glm(data, glm.fit)$delta[1]
}
cv.err
# You will note a sharp drop in the estimated test MSE between the linear and quadratic fits.
# The quadratic fit is the best having a lower MSE.
# Thus:
train.control <- trainControl(method = "LOOCV")
LOOCV_fit <- train(mpg ~ poly(horsepower,2), data = data,
method = "lm",
trControl = train.control)
LOOCV_fit
# The advantage of the LOOCV method is that we make use all data points reducing potential bias.
# k-Fold Cross-Validation
# _______________________
# k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV
set.seed(17)
cv.err.10 <- rep(0, 10)
for(i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data = data)
cv.err.10[i] <- cv.glm(data, glm.fit, K= 10)$delta[1]
}
cv.err.10
# The quadratic fit to the the 9th place it the best fit having the lowest MSE. Thus:
train.control2 <- trainControl(method = "cv", number = 10)
kfold_fit <- train(mpg~poly(horsepower, 9), data = data,
method = "lm",
trControl = train.control2)
# The Bootstrap
#______________
# Estimating the Accuracy of a Linear Regression Model
#____________________________________________________
# 1. Create a simple function, which takes in the Auto data set as well as a set of index for the observations,
#     and returns regression coefficients:
boot.fn <- function(data, index)
return(coef(lm(mpg~horsepower, data = data[index,])))
#Apply the function
boot.fn(Auto, 1:392)
# 2. Compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.
boot(Auto, boot.fn, 1000)
